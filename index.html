<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Visual Instruction Tuning">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VEGA</title>
  <style>
    #painting_icon {
      position: relative;
      top: -20px; /* 调整此值以控制图片上移的程度 */
      margin-left: 5px; /* 调整此值以控制图片与文字之间的间距 */
    }
    #painting_icon2 {
      position: relative;
      top: -20px; /* 调整此值以控制图片上移的程度 */
      margin-left: 0px; /* 调整此值以控制图片与文字之间的间距 */
    }
  </style>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/icon.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>

  <style>
    .expandable-card .card-text-container {
        max-height: 200px;
        overflow-y: hidden;
        position: relative;
    }
    .expandable-card.expanded .card-text-container {
        max-height: none;
    }
    .expand-btn {
        position: relative;
        display: none;
        background-color: rgba(255, 255, 255, 0.8);
        color: #510c75;
        border-color: transparent;
    }
    .expand-btn:hover {
        background-color: rgba(200, 200, 200, 0.8);
        text-decoration: none;
        border-color: transparent;
        color: #510c75;
    }
    .expand-btn:focus {
        outline: none;
        text-decoration: none;
    }
    .expandable-card:not(.expanded) .card-text-container:after {
        content: "";
        position: absolute;
        bottom: 0;
        left: 0;
        width: 100%;
        height: 90px;
        background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
    }
    .expandable-card:not(.expanded) .expand-btn {
        margin-top: -40px;
    }
    .card-body {
        padding-bottom: 5px;
    }
    .vertical-flex-layout {
        justify-content: center;
        align-items: center;
        height: 100%;
        display: flex;
        flex-direction: column;
        gap: 5px;
    }
    .figure-img {
        max-width: 100%;
        height: auto;
    }
    .adjustable-font-size {
        font-size: calc(0.5rem + 2vw);
    }
    .chat-history {
        flex-grow: 1;
        overflow-y: auto;
        padding: 5px;
        border-bottom: 1px solid #ccc;
        margin-bottom: 10px;
    }
    #gradio pre {
        background-color: transparent;
    }
    /* 独特命名空间的滑动窗口样式 */
/* 独特命名空间的滑动窗口样式 */
.image-slider {
  position: relative;
  width: 100%; /* 调整宽度 */
  max-width: 5000px; /* 调整最大宽度 */

  overflow: hidden;
  border-radius: 10px;
  box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
  margin: 20px auto;
}

.image-slider .slides {
  display: flex;
  transition: transform 0.5s ease-in-out;
  height: 100%; /* 确保slides高度填满父元素 */
}

.image-slider .slide {
  min-width: 100%;
  box-sizing: border-box;
  height: 100%; /* 确保slide高度填满父元素 */
}

.image-slider .slide img {
  width: 100%;
  height: 100%; /* 确保图片高度填满父元素 */
  object-fit: cover; /* 保持图片的比例 */
  display: block;
  border-radius: 10px 10px 0 0;
}

.image-slider .caption {
  padding: 15px;
  text-align: center;
  font-size: 16px;
  color: #555;
  background-color: #fff;
  border-radius: 0 0 10px 10px;
  box-shadow: 0 -4px 10px rgba(0, 0, 0, 0.1);
}

.image-slider .navigation {
  position: absolute;
  top: 50%;
  width: 100%;
  display: flex;
  justify-content: space-between;
  transform: translateY(-50%);
}

.image-slider .navigation button {
  background-color: rgba(0, 0, 0, 0.5);
  border: none;
  color: white;
  width: 50px; /* 设置按钮的宽度 */
  height: 50px; /* 设置按钮的高度 */
  cursor: pointer;
  border-radius: 50%;
  outline: none;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: background-color 0.3s ease;
}

.image-slider .navigation button:hover {
  background-color: rgba(0, 0, 0, 0.8);
}


</style>
</head>



<body>

  <script src="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/js/bulma-carousel.min.js"></script>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-1 publication-title">VEGA<img id="painting_icon" width="3.5%" src="assets/lyra.png">: Learning Interleaved Image-Text Comprehension in Vision-Language Large Models</h2>
            <!-- <h3 class="title is-3 publication-title">Visual Instruction Tuning</h3> -->
            <!-- <h5 class="subtitle is-5 publication-awards">NeurIPS 2023 (Oral)</h5> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a  style="color:#f68946;font-weight:normal;">Chenyu Zhou<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a  style="color:#008AD7;font-weight:normal;">Mengdan Zhang<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a  style="color:#008AD7;font-weight:normal;">Peixian Chen <sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a  style="color:#008AD7;font-weight:normal;">Chaoyou Fu</a>,
              </span>
              <span class="author-block">
                <a  style="color:#008AD7;font-weight:normal;">Yunhang Shen</a>,
              </span>
              <span class="author-block">
                <a  style="color:#f68946;font-weight:normal;">Xiawu Zheng &dagger;</a>,
              </span>
              <span class="author-block">
                <a  style="color:#008AD7;font-weight:normal;">Xing Sun</a>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> Xiamen University</span>
              <!-- <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Tencent Youtu Lab</span> -->
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution</span>
              <span class="author-block"><sup>&dagger;</sup>Corresponding Author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <!-- TODO -->
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/zhourax/VEGA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section"  style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="font-family:Times New Roman">
              The swift progress of Multi-modal Large Models (MLLMs) has showcased their impressive ability to tackle tasks blending vision and language.
              Yet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual contexts.
              These models often fall short when faced with complex comprehension tasks, which involve navigating through a plethora of irrelevant and potentially misleading information in both text and image forms.
              To bridge this gap, we introduce a new, more demanding task known as Interleaved Image-Text Comprehension (IITC).
              This task challenges models to discern and disregard superfluous elements in both images and text to accurately answer questions and to follow intricate instructions to pinpoint the relevant image.
              In support of this task, we further craft a new VEGA dataset, tailored for the IITC task on scientific content, and devised a subtask, Image-Text Association (ITA), to refine image-text correlation skills.
              Our evaluation of four leading closed-source models, as well as various open-source models using VEGA, underscores the rigorous nature of IITC. 
              Even the most advanced models, such as Gemini-1.5-pro and GPT4V, only achieved modest success.
              By employing a multi-task, multi-scale post-training strategy, we have set a robust baseline for MLLMs on the IITC task, attaining an 85.8% accuracy rate in image association and a 0.508 Rouge score. These results validate the effectiveness of our dataset in improving MLLMs capabilities for nuanced image-text comprehension.
           </p>
  
          </div>
        </div>
      </div>
        
    </div>
  </section>


 

<section class="section">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"> Introduction </h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">

  <div class="columns is-centered">
    <div class="column is-full-width">
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="95%" src="assets/intro1.png">     
        </div>
      </centering>    
      <br>
      <div class="content has-text-justified"> 
        <p style="font-family:Times New Roman; text-align: left;">
          Comparison between existing VQA tasks and our IITC task.
          <ul type="1">
            <li><b>Left</b>: <span style="font-family:Times New Roman; text-align: left;">The input for existing VQA tasks only incorporates a limited amount of image and text data, which is highly relevant to the question.</span></li>
            <li><b>Right</b>: <span style="font-family:Times New Roman; text-align: left;">The input for the IITC task contains longer images and text information, which includes redundant and misleading data. The model needs to specify the reference image when providing an answer.
          </ul>
        </p>
        <p style="font-family:Times New Roman; text-align: left;">
          To evaluate and enhance model performance on the IITC task, we have created the <strong>VEGA Dataset</strong>, which focuses on the <strong>comprehension of scientific papers</strong> and contains over <strong>50,000</strong> scientific articles. The VEGA dataset is structured into two subsets, each specifically curated to train models on the <strong>IITC and ITA tasks</strong>, respectively. The ITA task is a subtask designed to support the training of the IITC task. We fine-tuned the Qwen-VL-Chat model on the VEGA dataset using a <strong>multi-scale, multi-task</strong> training strategy, resulting in the <strong>VEGA-Base model</strong>. Our experimental results show that this model achieves an image association accuracy rate of <strong>85.8%</strong>, which is a substantial improvement and a strong baseline for the IITC task.
        </p>
        <centering>
          <div style="text-align: center;">
            <img id="teaser" width="95%" src="assets/intro2.png">     
          </div>
        </centering>
      </p>
      <p style="font-family:Times New Roman; text-align: left;">
        The task definition of IITC and ITA tasks. (a) The IITC task takes long interleaved image-text content as input and requires the model to specify the image it refers to in its response. (b) The ITA task takes shuffled images and text segments from different articles as input and requires the model to output the relationship between the text and the images. &ltText *&gt and &ltImage *&gt represent a text segment and an image, respectively. They are both tokenized and fed into the model along with the task prompt and the question.
      </p>
      </div>       
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Examples of VEGA <img id="painting_icon2" width="3%" src="assets/lyra.png"> Dataset</h2>
        <p>All papers in VEGA are from arxiv.</p>
      </div>
    </div>
    <div class="image-slider slider-1">
      <div class="slides">
        <div class="slide">
          <img src="assets/IITC_case1.jpg" alt="IITC Case 1">
          <div class="caption">IITC Case 1</div>
        </div>
        <div class="slide">
          <img src="assets/IITC_case2.jpg" alt="IITC Case 2">
          <div class="caption">IITC Case 2</div>
        </div>
        <div class="slide">
          <img src="assets/ITA_case1.jpg" alt="Image 3">
          <div class="caption">ITA Case 1</div>
        </div>
      </div>
      <div class="navigation">
        <button class="prev">❮</button>
        <button class="next">❯</button>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Specific Cases of VEGA-Base <img id="painting_icon2" width="3%" src="assets/lyra.png"> </h2>
        <p>All papers in VEGA are from arxiv.</p>
      </div>
    </div>
    <div class="image-slider slider-2">
      <div class="slides">
        <div class="slide">
          <img src="assets/VEGA_Base_case1.jpg" alt="IITC Case 1">
          <div class="caption">IITC Case 1</div>
        </div>
        <div class="slide">
          <img src="assets/VEGA_Base_case2.jpg" alt="IITC Case 2">
          <div class="caption">IITC Case 2</div>
        </div>
        <!-- <div class="slide">
          <img src="/Users/zhouchenyu/Desktop/科研/优图实验室/多模态文档理解/VEGA/assets/ITA_case1.jpg" alt="Image 3">
          <div class="caption">ITA Case 1</div>
        </div> -->
      </div>
      <div class="navigation">
        <button class="prev">❮</button>
        <button class="next">❯</button>
      </div>
    </div>
  </div>
</section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
TODO
  </code></pre>
    </div>
  </section>
  
  

  <script>
// document.addEventListener('DOMContentLoaded', () => {
//   // Handle message showing
//   function createChatRow(sender, text, imageSrc) {
//     var article = document.createElement("article");
//     article.className = "media";

//     var figure = document.createElement("figure");
//     figure.className = "media-left";

//     var span = document.createElement("span");
//     span.className = "icon is-large";

//     var icon = document.createElement("i");
//     icon.className = "fas fa-2x" + (sender === "User" ? " fa-user " : sender === "LLaVA" ? " fa-robot" : "");

//     var media = document.createElement("div");
//     media.className = "media-content";

//     var content = document.createElement("div");
//     content.className = "content";

//     var para = document.createElement("p");

//     // wrap text in pre tag to preserve whitespace and line breaks
//     var pre_text = document.createElement("pre");
//     pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
//     var paraText = document.createTextNode(text);
//     pre_text.appendChild(paraText);

//     var strong = document.createElement("strong");
//     strong.innerHTML = sender;
//     var br = document.createElement("br");

//     para.appendChild(strong);
//     para.appendChild(br);
//     para.appendChild(pre_text);

//     // Add image if imageSrc is provided
//     if (imageSrc) {
//       var img = document.createElement("img");
//       img.src = imageSrc;
//       img.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
//       para.appendChild(img);
//     }

//     content.appendChild(para);
//     media.appendChild(content);
//     span.appendChild(icon);
//     figure.appendChild(span);
//     if (sender !== "Description") {
//       article.appendChild(figure);
//     }
//     article.appendChild(media);
//     return article;
//   }

//   function addMessageToChatHistory(sender, message, imageSrc) {
//     const chatHistory = document.querySelector('.chat-history');
//     const chatRow = createChatRow(sender, message, imageSrc);
//     chatHistory.appendChild(chatRow);
//     chatHistory.scrollTop = chatHistory.scrollHeight;
//   }

//   function clearChatHistory() {
//     const chatHistory = document.querySelector('.chat-history');
//     chatHistory.innerHTML = "";
//   }

//   // The current image index
//   let currentIndex = 0;

//   // The function to update the displayed chat history
//   function update_dialog_demo() {
//     // Clear the chat history
//     clearChatHistory();

//     for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
//       if (conversations[currentIndex].turns[i].length == 2) {
//         addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
//       } else {
//         addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
//       }
//     }

//     // scroll to the top of the chat history
//     document.querySelector('.chat-history').scrollTop = 0;
//   }

//   // Initialize the displayed image
//   update_dialog_demo();

//   // Event listeners for the buttons
//   document.getElementById('prev-question').addEventListener('click', () => {
//     currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
//     update_dialog_demo();
//   });

//   document.getElementById('next-question').addEventListener('click', () => {
//     currentIndex = (currentIndex + 1) % conversations.length;
//     update_dialog_demo();
//   });

  // 滑动窗口的代码
  document.addEventListener('DOMContentLoaded', () => {
  function initSlider(sliderClass) {
    const slider = document.querySelector(`.${sliderClass}`);
    const slides = slider.querySelector('.slides');
    const slide = slider.querySelectorAll('.slide');
    let currentSlideIndex = 0;

    slider.querySelector('.next').addEventListener('click', () => {
      currentSlideIndex = (currentSlideIndex + 1) % slide.length;
      updateSlide();
    });

    slider.querySelector('.prev').addEventListener('click', () => {
      currentSlideIndex = (currentSlideIndex - 1 + slide.length) % slide.length;
      updateSlide();
    });

    function updateSlide() {
      slides.style.transform = `translateX(-${currentSlideIndex * 100}%)`;
    }
  }

  initSlider('slider-1');
  initSlider('slider-2');
});


  </script>

</body>

</html>